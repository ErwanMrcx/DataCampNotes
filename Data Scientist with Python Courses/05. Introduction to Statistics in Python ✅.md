# Chapter 1 - Introduction to statistics in python

### Table of Contents

1. Summary Statistics
    - Mean and Median
2. Quartiles, Quantiles, and Quintiles
3. Variance and Standard Deviation
4. Finding Outliers using IQR

### Mean and Median

In this chapter, we will explore measures of center, such as mean and median, to compare food consumption in different countries. We will use the `food_consumption` dataset, which contains information about the kilograms of food consumed per person per year in each country and the carbon footprint of that food category.

```python
# Import necessary libraries
import numpy as np
import pandas as pd

# Read the food_consumption dataset
food_consumption = pd.read_csv('datasets/food_consumption.csv')

# Filter for Belgium
be_consumption = food_consumption[food_consumption['country'] == 'Belgium']

# Filter for USA
usa_consumption = food_consumption[food_consumption['country'] == 'USA']

# Calculate mean and median consumption in Belgium
print(np.mean(be_consumption['consumption']))
print(np.median(be_consumption['consumption']))

# Calculate mean and median consumption in USA
print(np.mean(usa_consumption['consumption']))
print(np.median(usa_consumption['consumption']))

```

### Quartiles, Quantiles, and Quintiles

Quantiles are useful for summarizing numerical data and understanding the distribution of the data. In this section, we will calculate quartiles, quintiles, and deciles to split up the data into different portions.

```python
# Calculate the quartiles of co2_emission
print(np.quantile(food_consumption['co2_emission'], [0, 0.25, 0.5, 0.75, 1]))

# Calculate the quintiles of co2_emission
print(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1]))

# Calculate the deciles of co2_emission
print(np.quantile(food_consumption['co2_emission'], np.linspace(0, 1, 11)))

```

### Variance and Standard Deviation

Variance and standard deviation are measures of spread that provide insights into the variability of the data. In this section, we will calculate the variance and standard deviation of the carbon emissions for each food category.

```python
# Calculate the variance and standard deviation of co2_emission for each food category
print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))

```

### Finding Outliers using IQR

Outliers can significantly affect statistical measures. In this section, we will use the interquartile range (IQR) to identify outliers in the total CO2 emissions per country.

```python
# Calculate total co2_emission per country
emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()

# Compute the first and third quartiles and IQR of emissions_by_country
q1 = np.quantile(emissions_by_country, 0.25)
q3 = np.quantile(emissions_by_country, 0.75)
iqr = q3 - q1

# Calculate the lower and upper cutoffs for outliers
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

# Subset emissions_by_country to find outliers
outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]
print(outliers)

```

Congratulations on completing the course on Statistics in Python

! This course covered various statistical measures, including mean, median, quartiles, quantiles, variance, standard deviation, and outlier detection using the interquartile range. By mastering these concepts, you will be able to analyze and interpret data more effectively. Keep practicing and exploring new techniques to enhance your statistical analysis skills!

# Chapter 2 - Random Numbers and Probability

### Table of Contents

1. Calculating Probabilities
2. Sampling Deals
3. Creating a Probability Distribution
4. Identifying Distributions
5. Expected Value vs. Sample Mean
6. Data Back-ups
7. Simulating Wait Times
8. Simulating Sales Deals
9. Calculating Binomial Probabilities
10. How Many Sales Will be Won?

### Calculating Probabilities

In this section, we will explore how to calculate probabilities using the count of deals Amir worked on for each product type.

```
import pandas as pd

amir_deals = pd.read_csv('datasets/amir_deals.csv')

# Count the deals for each product
counts = amir_deals['product'].value_counts()
print(counts)

# Calculate probability of picking a deal with each product
probs = counts / amir_deals.shape[0]
print(probs)

```

### Sampling Deals

Now, let's simulate the process of sampling deals randomly from Amir's work. We will take samples with and without replacement.

```
import numpy as np

# Set random seed
np.random.seed(24)

# Sample 5 deals without replacement
sample_without_replacement = amir_deals.sample(5)
print(sample_without_replacement)

# Sample 5 deals with replacement
sample_with_replacement = amir_deals.sample(5, replace=True)
print(sample_with_replacement)

```

### Creating a Probability Distribution

We will create a probability distribution based on the group sizes of customers waiting at a restaurant.

```
import matplotlib.pyplot as plt

# Create a histogram of restaurant_groups and show plot
restaurant_groups['group_size'].hist(bins=np.linspace(2, 6, 5))
plt.show()

# Create probability distribution
size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]
size_dist = size_dist.reset_index()
size_dist.columns = ['group_size', 'prob']
print(size_dist)

# Expected value
expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])
print(expected_value)

# Probability of selecting a group of 4 or more people
groups_4_or_more = size_dist[size_dist['group_size'] >= 4]
prob_4_or_more = np.sum(groups_4_or_more['prob'])
print(prob_4_or_more)

```

### Identifying Distributions

We will identify the most likely sample to have been taken from a uniform distribution.

### Expected Value vs. Sample Mean

We will compare the expected value of a discrete uniform distribution with the sample mean.

### Data Back-ups

We will use the continuous uniform distribution to model the waiting time for data back-ups.

```
# Min and max wait times for back-up that happens every 30 min
min_time = 0
max_time = 30

# Import uniform from scipy.stats
from scipy.stats import uniform

# Calculate probability of waiting less than 5 mins
prob_less_than_5 = uniform.cdf(5, min_time, max_time)
print(prob_less_than_5)

# Calculate probability of waiting more than 5 mins
prob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)
print(prob_greater_than_5)

# Calculate probability of waiting 10-20 mins
prob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)
print(prob_between_10_and_20)

```

### Simulating Wait Times

We will simulate Amir's wait times for data back-ups and visualize the results using a histogram.

```
# Set random

 seed to 334
np.random.seed(334)

# Import uniform
from scipy.stats import uniform

# Generate 1000 wait times between 0 and 30 mins
wait_times = uniform.rvs(0, 30, size=1000)

# Create a histogram of simulated times and show plot
plt.hist(wait_times)
plt.show()

```

### Simulating Sales Deals

We will simulate the outcomes of Amir's sales deals using the binomial distribution.

```
# Import binom from scipy.stats
from scipy.stats import binom

# Set random seed to 10
np.random.seed(10)

# Simulate a single deal
print(binom.rvs(1, 0.3, size=1))

# Simulate 1 week of 3 deals
print(binom.rvs(3, 0.3, size=1))

# Simulate 52 weeks of 3 deals
deals = binom.rvs(3, 0.3, size=52)

# Print mean deals won per week
print(np.mean(deals))

```

### Calculating Binomial Probabilities

We will calculate the probabilities of different outcomes using the binomial distribution.

```
# Probability of closing 3 out of 3 deals
prob_3 = binom.pmf(3, 3, 0.3)
print(prob_3)

# Probability of closing <= 1 deal out of 3 deals
prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)
print(prob_less_than_or_equal_1)

# Probability of closing > 1 deal out of 3 deals
prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)
print(prob_greater_than_1)

```

### How Many Sales Will be Won?

We will calculate the expected number of sales won each week under different win rates.

```
# Expected number won with 30% win rate
won_30pct = 3 * 0.3
print(won_30pct)

# Expected number won with 25% win rate
won_25pct = 3 * 0.25
print(won_25pct)

# Expected number won with 35% win rate
won_35pct = 3 * 0.35
print(won_35pct)

```

Feel free to explore and expand upon these topics to further enhance your understanding of random numbers and probability!

# Chapter 3 - More Distributions and the Central Limit Theorem

### Table of Contents:

1. Distribution of Amir's sales
2. Probabilities from the normal distribution
3. Simulating sales under new market conditions
4. Which market is better?
5. Visualizing sampling distributions
6. The CLT in action
7. The mean of means
8. Tracking lead responses
9. Modeling time between leads
10. The t-distribution

---

To estimate the probability of Amir selling different amounts, we need to determine the distribution of the sales amounts. Let's visualize the distribution using a histogram.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

amir_deals = pd.read_csv('datasets/amir_deals.csv')

# Histogram of amount with 10 bins and show plot
amir_deals['amount'].hist(bins=10)
plt.show()

```

Which probability distribution do the sales amounts most closely follow?

- Uniform
- Binomial
- Normal
- None of the above
1. Probabilities from the normal distribution

---

Since the sales amounts follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars, we can calculate probabilities using the normal distribution.

```python
from scipy.stats import norm

# Probability of deal < 7500
prob_less_7500 = norm.cdf(7500, 5000, 2000)
print(prob_less_7500)

# Probability of deal > 1000
prob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)
print(prob_over_1000)

# Probability of deal between 3000 and 7000
prob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)
print(prob_3000_to_7000)

# Amount that 25% of deals will be less than
pct_25 = norm.ppf(0.25, 5000, 2000)
print(pct_25)

```

1. Simulating sales under new market conditions

---

To simulate sales under new market conditions, where the average sale amount increases by 20% and the standard deviation increases by 30%, we can use the normal distribution.

```python
# Calculate new average amount
new_mean = 5000 * 1.2

# Calculate new standard deviation
new_sd = 2000 * 1.3

# Simulate 36 new sales
new_sales = norm.rvs(new_mean, new_sd, size=36)

# Create histogram and show
plt.hist(new_sales)
plt.show()

```

1. Which market is better?

---

To evaluate the markets based on the metric of the percent of sales over $1000, we compare the current market with Amir's sales having a mean of 2000 and the predicted market with a mean of 2600.

```python
# Calculate the probability of sales over $1000 in the current market
prob_current = norm.cdf(1000, 2000, 2000)
print(prob_current)

# Calculate the probability of sales over $1000 in the predicted market
prob_predicted = norm.cdf(1000, 2600, 2000)
print(prob_predicted)

# Compare the probabilities to determine which market is better
if prob_current > prob_predicted:
    print("Amir performs better in the current market.")
elif prob_current < prob_predicted:
    print("Amir performs better in the

 predicted market.")
else:
    print("Amir performs about equally in both markets.")

```

1. Visualizing sampling distributions

---

To visualize the sampling distributions, we can create histograms of different summary statistics from samples of different distributions. The central limit theorem applies to all distributions except the discrete uniform distribution.

1. The CLT in action

---

The central limit theorem (CLT) states that the sampling distribution of a sample statistic approaches the normal distribution as the sample size increases, regardless of the original distribution being sampled from. We'll observe the CLT by examining the num_users column of amir_deals.

```python
# Create a histogram of num_users
amir_deals['num_users'].hist()
plt.show()

# Set the seed to 104
np.random.seed(104)

# Take a sample of size 20 with replacement from num_users
samp_20 = amir_deals['num_users'].sample(20, replace=True)

# Take the mean of samp_20
print(np.mean(samp_20))

# Repeat 100 times and store sample means
sample_means = []
for i in range(100):
    samp_20 = amir_deals['num_users'].sample(20, replace=True)
    samp_20_mean = np.mean(samp_20)
    sample_means.append(samp_20_mean)

# Convert sample_means to a Series and plot histogram
sample_means_series = pd.Series(sample_means)
sample_means_series.hist()
plt.show()

```

1. The mean of means

---

To estimate the average number of users per deal for the entire company, we can take several random samples of deals. This allows us to approximate the mean without collecting data from everyone in the company.

```python
# Set the random seed to 321
np.random.seed(321)

# Take 30 samples (with replacement) of size 20 from all_deals['num_users'] and take the mean of each sample
sample_means = []
for i in range(30):
    cur_sample = amir_deals['num_users'].sample(20, replace=True)
    cur_mean = np.mean(cur_sample)
    sample_means.append(cur_mean)

# Print the mean of sample_means
print(np.mean(sample_means))

# Print the mean of the num_users column of amir_deals
print(np.mean(amir_deals['num_users']))

```

1. Tracking lead responses

---

To calculate probabilities related to the number of lead responses, we can use the Poisson distribution. Given that Amir responds to an average of 4 leads each day, we can calculate the probabilities as follows:

```python
from scipy.stats import poisson

# Probability of Amir responding to 5 leads in a day
prob_5 = poisson.pmf(5, 4)
print(prob_5)

# Probability of Amir's coworker answering 5 leads in a day
prob_coworker = poisson.pmf(5, 5.5)
print(prob_coworker)

# Probability of Amir responding to 2 or fewer leads in a day
prob_2_or_less = poisson.cdf(2, 4)
print(prob_2_or_less)

# Probability of Amir responding to more than 10 leads in a day
prob_over_10 = 1 - poisson.cdf(10, 4)
print(prob_over_10)

```

1. Modeling time between leads

---

To model the time between leads, we can use the exponential distribution. Given that Amir takes an average of 2.5 hours to respond to a lead, we can calculate the probabilities as follows:

```python
from scipy.stats import expon

# Probability that Amir takes less than an

 hour to respond
prob_less_1_hour = expon.cdf(1, scale=2.5)
print(prob_less_1_hour)

# Probability that Amir takes more than 4 hours to respond
prob_over_4_hours = 1 - expon.cdf(4, scale=2.5)
print(prob_over_4_hours)

# Probability that Amir takes 3-4 hours to respond
prob_3_to_4_hours = expon.cdf(4, scale=2.5) - expon.cdf(3, scale=2.5)
print(prob_3_to_4_hours)

```

1. The t-distribution

---

The t-distribution is similar to the normal distribution but has thicker tails and higher variance. It is used when the population standard deviation is unknown and the sample size is small. All the statements about the t-distribution mentioned above are true except that it is skewed. The t-distribution is not skewed.

# Chapter 4 - Correlation and Experimental Design

### Table of Contents

1. Guess the correlation
2. Relationships between variables
3. What can't correlation measure?
4. Transforming variables
5. Does sugar improve happiness?
6. Confounders
7. Longitudinal vs. cross-sectional studies

### 1. Guess the correlation

On the right, use the scatterplot to estimate what the correlation is between the variables x and y. Once you've guessed it correctly, use the New Plot button to try out a few more scatterplots. When you're ready, answer the question below to continue to the next exercise.

```python
# No code to provide. Exercise is based on visual estimation.

```

### 2. Relationships between variables

In this chapter, you'll be working with a dataset `world_happiness` containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

world_happiness = pd.read_csv('datasets/world_happiness.csv')

# Create a scatterplot of happiness_score vs. life_exp (without a trendline) using seaborn
sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)

# Show plot
plt.show()

# Create a scatterplot of happiness_score vs. life_exp with a linear trendline using seaborn
sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)

# Show plot
plt.show()

# Calculate the correlation between life_exp and happiness_score
cor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])
print(cor)

```

### 3. What can't correlation measure?

While the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it's far from perfect. In this exercise, you'll explore one of the caveats of the correlation coefficient by examining the relationship between a country's GDP per capita (gdp_per_cap) and happiness score.

```python
# Scatterplot of gdp_per_cap and life_exp
sns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)

# Show plot
plt.show()

# Calculate the correlation between gdp_per_cap and life_exp
cor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])
print(cor)

```

### 4. Transforming variables

When variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. In this exercise, you'll perform a transformation yourself.

```python
# Scatterplot of happiness_score versus gdp_per_cap and calculate the correlation between them
sns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)
plt.show()

# Calculate correlation
cor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])
print(cor)

# Add a new column to world_happiness called log_gdp_per_cap that contains the log of gdp_per_cap
import numpy as np
world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])

# Scatterplot of log_gdp_per_cap and happiness_score and calculate the correlation between them
sns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)
plt.show()

# Calculate correlation
cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])
print(cor)

```

### 5. Does sugar improve happiness?

A new column has been added to `world_happiness` called `grams_sugar_per_day`, which contains the average amount of sugar eaten per person per day in each country. In this exercise, you'll examine the effect of a country's average sugar consumption on its happiness score.

```python
# Scatterplot of grams_sugar_per_day and happiness_score
sns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)
plt.show()

# Correlation between grams_sugar_per_day and happiness_score
cor = world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])
print(cor)

```

### 6. Confounders

A study is investigating the relationship between neighborhood residence and lung capacity. Researchers measure the lung capacity of thirty people from neighborhood A, which is located near a highway, and thirty people from neighborhood B, which is not near a highway. Both groups have similar smoking habits and a similar gender breakdown.

```python
# No code to provide. Exercise is based on understanding confounders.

```

### 7. Longitudinal vs. cross-sectional studies

A company manufactures thermometers, and they want to study the relationship between a thermometer's age and its accuracy. To do this, they take a sample of 100 different thermometers of different ages and test how accurate they are. Is this data longitudinal or cross-sectional?

```python
# No code to provide. Exercise is based on understanding longitudinal and cross-sectional studies.

```