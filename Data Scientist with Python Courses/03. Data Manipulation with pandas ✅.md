# Chapter 1 - Transforming Data

## Inspecting a DataFrame

### .head()

To get a glimpse of the first few rows of a DataFrame, you can use the `.head()` method. It returns the "head" of the DataFrame, which is the first few rows.

```python
import pandas as pd

homelessness = pd.read_csv('datasets/homelessness.csv')

# Print the head of the homelessness DataFrame
print(homelessness.head())
print(homelessness.tail())

```
### .info()

The `.info()` method provides information about the columns in a DataFrame, such as data type and the number of missing values.

```python
# Print information about column types and missing values in homelessness
print(homelessness.info())

```

### .shape

The `.shape` attribute returns the number of rows and columns in a DataFrame.

```python
# Print the number of rows and columns in homelessness
print(homelessness.shape)

```

### .describe()

The `.describe()` method calculates summary statistics for each column in a DataFrame.

```python
# Print summary statistics for homelessness
print(homelessness.describe())

```

### Parts of a DataFrame

To understand DataFrame objects better, it's helpful to know their three main components:

- `values`: A two-dimensional NumPy array containing the values.
- `columns`: An index of column names.
- `index`: An index for the rows, usually consisting of row numbers or row names.

You can access these components as attributes of a DataFrame.

```python
# Print the values of homelessness
print(homelessness.values)

# Print the column index of homelessness
print(homelessness.columns)

# Print the row index of homelessness
print(homelessness.index)

```

### Sorting Rows

To explore interesting data patterns in a DataFrame, it's often useful to change the order of rows. You can sort rows based on one or more columns using the `.sort_values()` method. Sorting can be done in ascending or descending order.

```python
# Sort homelessness by the number of homeless individuals (smallest to largest)
homelessness_ind = homelessness.sort_values('individuals')

# Print the top few rows
print(homelessness_ind.head())

# Sort homelessness by the number of homeless family_members (largest to smallest)
homelessness_fam = homelessness.sort_values('family_members', ascending=False)

# Print the top few rows
print(homelessness_fam.head())

# Sort homelessness by region (ascending) and then by number of family_members (descending)
homelessness_reg_fam = homelessness.sort_values(["region", "family_members"], ascending=[True, False])

# Print the top few rows
print(homelessness_reg_fam.head())

```

### Subsetting Columns

To work with a subset of columns in a DataFrame, you can use square brackets `[]` to select the desired columns. You can select a single column or multiple columns by providing a list of column names.

```python
# Select the individuals column
individuals = homelessness['individuals']

# Print the head of the result
print(individuals.head())

# Select the state and family_members columns
state_fam = homelessness[['state', 'family_members']]

# Print the head of the result
print(state_fam.head())

# Select the individuals and state columns
ind_state = homelessness[['individuals', 'state']]

# Print the head of the result
print(ind_state.head())

```

### Subsetting Rows

To filter rows in a DataFrame based on specific conditions, you can use conditional statements within square brackets `[]`. By combining multiple conditions using logical operators such as `&` (and) and `|` (or), you can create more complex filters.

```python
# Filter for rows where individuals is greater than 10,000
ind_gt_10k = homelessness[homelessness['individuals'] > 10000]

# See the result
print(ind_gt_10k)

# Filter for rows where the USA Census region is "Mountain"
mountain_reg = homelessness[homelessness['region'] == 'Mountain']

# See the result
print(mountain_reg)

# Filter for rows where family_members is less than 1,000 and region is "Pacific"
fam_lt_1k_pac = homelessness[(homelessness['family_members'] < 1000) & (homelessness['region'] == 'Pacific')]

# See the result
print(fam_lt_1k_pac)

```

### Subsetting Rows by Categorical Variables

When subsetting rows based on categorical variables, using the `isin()` method can be more efficient than writing multiple conditions. The `isin()` method checks if a value is present in a list of values and returns a Boolean Series that can be used for subsetting.

```python
# Subset for rows in "South Atlantic" or "Mid-Atlantic" regions
south_mid_atlantic = homelessness[homelessness['region'].isin(['South Atlantic', 'Mid-Atlantic'])]

# See the result
print(south_mid_atlantic)

# The Mojave Desert states
canu = ["California", "Arizona", "Nevada", "Utah"]

# Filter for rows in the Mojave Desert states
mojave_homelessness = homelessness[homelessness['state'].isin(canu)]

# See the result
print(mojave_homelessness)

```

### Adding New Columns

You can add new columns to a DataFrame by assigning values to a new column name within square brackets `[]`. New columns can be created from scratch or derived from existing columns.

```python
# Add a new column named "total" containing the sum of individuals and family_members
homelessness['total'] = homelessness['individuals'] + homelessness['family_members']

# Add a new column named "p_individuals" containing the proportion of individuals out of the total
homelessness['p_individuals'] = homelessness['individuals'] / homelessness['total']

# See the result
print(homelessness)

```

### Combo-Attack!

By combining sorting, subsetting columns, subsetting rows, and adding new columns, you can answer complex questions about the data. Here's an example that finds the state with the highest number of homeless individuals per 10,000 people.

```python
# Create a column named "indiv_per_10k" as the number of homeless individuals per 10,000 state population
homelessness['indiv_per_10k'] = (10000 * homelessness['individuals']) / homelessness['state_pop']

# Subset for rows where indiv_per_10k is higher than 20
high_homelessness = homelessness[homelessness['indiv_per_10k'] > 20]

# Sort high_homelessness by descending indiv_per_10k
high_homelessness_srt = high_homelessness.sort_values('indiv_per_10k', ascending=False)

# Select only the state and indiv_per_10k columns of high_homelessness_srt
result = high_homelessness_srt[['state', 'indiv_per_10k']]

# See the result
print(result)

```

By applying these techniques, you can manipulate and explore data effectively using pandas.

# Chapter 2 - Aggregating Data

### Mean and Median

Summary statistics are useful for summarizing large amounts of data in a single statistic. They provide insights into the central tendency and spread of the data. In this lesson, you will learn how to calculate summary statistics such as the mean and median using pandas.

Let's start by exploring the sales DataFrame:

```python
import pandas as pd

sales = pd.read_csv('C:/Users/Александр/pj/DataCamp_projects/Data Scientist With Python/Data Manipulation with pandas Course 3/datasets/sales_subset.csv')

# Print the first few rows of the sales DataFrame
print(sales.head())

# Print information about the columns in sales
print(sales.info())

```

The above code reads a CSV file into a DataFrame called `sales` and then prints the first few rows and column information using the `head()` and `info()` methods, respectively.

To calculate the mean and median of the `weekly_sales` column, you can use the following code:

```python
# Print the mean of weekly_sales
print(sales["weekly_sales"].mean())

# Print the median of weekly_sales
print(sales["weekly_sales"].median())

```

The code above uses the `mean()` and `median()` methods on the `weekly_sales` column to calculate the mean and median, respectively.

### Summarizing Dates

Summary statistics can also be calculated on date columns, which provide insights into the time range of the data. In this lesson, you will learn how to calculate summary statistics on date columns using pandas.

To find the maximum and minimum dates in the `date` column, you can use the following code:

```python
# Print the maximum of the date column
print(sales.date.max())

# Print the minimum of the date column
print(sales.date.min())

```

The above code uses the `max()` and `min()` methods on the `date` column to find the maximum and minimum dates, respectively.

### Efficient Summaries

Sometimes, you may need to apply custom functions or multiple aggregation functions to calculate summary statistics efficiently. In this lesson, you will learn how to use the `.agg()` method to apply custom functions and multiple aggregation functions to a DataFrame.

To calculate the interquartile range (IQR) of the `temperature_c` column, you can define a custom function and use it with the `.agg()` method as follows:

```python
# A custom IQR function
def iqr(column):
    return column.quantile(0.75) - column.quantile(0.25)

# Print the IQR of the temperature_c column
print(sales["temperature_c"].agg(iqr))

```

The code above defines a custom function `iqr()` that calculates the IQR of a column. It then uses the `.agg()` method on the `temperature_c` column, passing the `iqr` function as an argument.

You can also apply multiple aggregation functions to different columns simultaneously. For example:

```python
import numpy as np

# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, and unemployment
print(sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg([iqr, np.median]))

```

The code above imports NumPy as `np` and applies the `iqr()` function and `np.median` function to the columns `temperature_c`, `fuel_price_usd_per_l`, and `unemployment` using the `.agg()` method.

### Cumulative Statistics

Cumulative statistics can provide insights into trends and patterns in data over time. In this lesson, you will learn how to calculate cumulative statistics using pandas.

To calculate the cumulative sum

and cumulative maximum of a department's weekly sales, you can use the following code:

```python
# Sort the rows of sales_1_1 by the date column in ascending order
sales_1_1 = sales_1_1.sort_values("date")

# Get the cumulative sum of weekly_sales and add it as a new column
sales_1_1["cum_weekly_sales"] = sales_1_1["weekly_sales"].cumsum()

# Get the cumulative maximum of weekly_sales and add it as a new column
sales_1_1["cum_max_sales"] = sales_1_1["weekly_sales"].cummax()

# Print the columns you calculated
print(sales_1_1[["date", "weekly_sales", "cum_weekly_sales", "cum_max_sales"]])

```

The code above first sorts the rows of the DataFrame `sales_1_1` by the `date` column in ascending order. Then, it calculates the cumulative sum and cumulative maximum of the `weekly_sales` column using the `cumsum()` and `cummax()` methods, respectively. Finally, it prints the columns `date`, `weekly_sales`, `cum_weekly_sales`, and `cum_max_sales` using indexing.

### Dropping Duplicates

Removing duplicates is important for accurate data analysis, as duplicate entries can skew summary statistics and counts. In this lesson, you will learn how to remove duplicates from a DataFrame using pandas.

To remove rows with duplicate pairs of `store` and `type` from the `sales` DataFrame, you can use the following code:

```python
# Remove rows with duplicate store/type combinations and save as store_types
store_types = sales.drop_duplicates(subset=["store", "type"])
print(store_types.head())

```

The code above uses the `drop_duplicates()` method on the `sales` DataFrame, specifying the columns `store` and `type` as the subset to consider for duplicates. The resulting DataFrame with removed duplicates is assigned to `store_types`, and the first few rows are printed using `head()`.

You can apply the same technique to remove duplicates based on other columns, such as `store` and `department`, as shown below:

```python
# Remove rows with duplicate store/department combinations and save as store_depts
store_depts = sales.drop_duplicates(subset=["store", "department"])
print(store_depts.head())

```

### Counting Categorical Variables

Counting categorical variables provides insights into the distribution and occurrence of different categories. In this lesson, you will learn how to count categorical variables using pandas.

To count the number of stores of each store type in the `store_types` DataFrame, you can use the following code:

```python
# Count the number of stores of each type
store_counts = store_types["type"].value_counts()
print(store_counts)

```

The code above uses the `value_counts()` method on the `type` column of the `store_types` DataFrame to count the occurrences of each store type.

To calculate the proportion of stores of each store type, you can use the following code:

```python
# Get the proportion of stores of each type
store_props = store_types["type"].value_counts(normalize=True)
print(store_props)

```

The code above divides the counts by the total number of stores to calculate the proportions.

To count the number of different departments in the `store_depts` DataFrame and sort the counts in descending order, you can use the following code:

```python
# Count the number of different departments and sort
dept_counts_sorted = store_depts["department"].value_counts(sort=True)
print(dept_counts_sorted)
```

The code above uses the `value_counts()` method on the `department` column of the `store_depts` DataFrame to count the occurrences of each department. By specifying `sort=True`, the resulting counts are sorted in descending order.

To calculate the proportion of different departments in the `store_depts` DataFrame and sort the proportions in descending order, you can use the following code:

```python
# Get the proportion of departments of each number and sort
dept_props_sorted = store_depts["department"].value_counts(sort=True, normalize=True)
print(dept_props_sorted)
```

The code above divides the counts by the total number of departments to calculate the proportions. By specifying `sort=True`, the resulting proportions are sorted in descending order.

### What Percent of Sales Occurred at Each Store Type?

To calculate the proportion of sales that occurred at each store type, you can use the following code:

```python
# Group by type and calculate total weekly sales
sales_by_type = sales.groupby("type")["weekly_sales"].sum()

# Calculate proportion of sales by type
sales_propn_by_type = sales_by_type / sum(sales_by_type)
print(sales_propn_by_type)

```

The code above groups the `sales` DataFrame by the `type` column and calculates the sum of `weekly_sales` for each store type using `.groupby()`. Then, it calculates the proportion of sales for each type by dividing the sales by the sum of all sales.

### Calculations with .groupby()

The `.groupby()` method in pandas allows you to perform grouped calculations on a DataFrame. In this lesson, you will learn how to use `.groupby()` to calculate summary statistics.

To calculate the total weekly sales for each store type using `.groupby()`, you can use the following code:

```python
# Group by type and calculate total weekly sales
sales_by_type = sales.groupby("type")["weekly_sales"].sum()
print(sales_by_type)

```

The code above groups the `sales` DataFrame by the `type` column and calculates the sum of `weekly_sales` for each store type.

To calculate the total weekly sales for each store type and whether it is a holiday week, you can use the following code:

```python
# Group by type and is_holiday and calculate total weekly sales
sales_by_type_is_holiday = sales.groupby(["type", "is_holiday"])["weekly_sales"].sum()
print(sales_by_type_is_holiday)

```

The code above groups the `sales` DataFrame by the `type` and `is_holiday` columns and calculates the sum of `weekly_sales` for each combination of store type and holiday status.

### Multiple Grouped Summaries

You can perform multiple summary calculations on grouped data using `.agg()` in pandas. In this lesson, you will learn how to calculate multiple statistics for each group.

To calculate the minimum, maximum, mean, and median of weekly sales for each store type, you can use the following code:

```python
# Import numpy as np
import numpy as np

# Pivot for mean weekly_sales for each store type
mean_sales_by_type = sales.pivot_table(values="weekly_sales", index="type")

# Print mean_sales_by_type
print(mean_sales_by_type)

```

The code above uses the `.pivot_table()` method to calculate the mean of `weekly_sales` for each store type.

To calculate the mean and median of weekly sales, as well as the minimum and maximum of unemployment and fuel price, for each store type, you can use the following code:

```python
# Pivot for mean and median weekly_sales for each store type
mean_med_sales_by_type = sales.pivot_table(values="weekly_sales", index="type", aggfunc=[np.mean, np.median])

# Print mean_med_sales_by_type
print(mean_med_sales_by_type)

```

The code above uses `.pivot_table()` with the `aggfunc` parameter set to `[np.mean, np.median]` to calculate both the mean and median of `weekly_sales` for each store type. Additionally, it calculates the minimum and maximum of `unemployment` and `fuel_price_usd_per_l` by default, as they are included in the `sales` DataFrame.

# Chapter 3 - Slicing and indexing dataframe

### Setting and Removing Indexes

- Setting an index allows for cleaner code and efficient lookup in pandas.
- To set an index in pandas, use the `.set_index()` method and specify the column(s) to be used as the index.
- To remove an index and restore the default integer index, use the `.reset_index()` method.
- The `.reset_index()` method can be used with the `drop=True` parameter to remove the index column.

Example:

```python
# Set the index of a DataFrame
df.set_index('column_name')

# Reset the index, keeping its contents
df.reset_index()

# Reset the index, dropping its contents
df.reset_index(drop=True)

```

### Subsetting with `.loc[]`

- `.loc[]` is a powerful subsetting method in pandas that accepts index values.
- It can be used to subset rows based on specific conditions or index labels.
- When using `.loc[]`, you can pass a single argument to subset rows.

Example:

```python
# Subsetting using square brackets
df[df['column'].isin(values)]

# Subsetting using .loc[]
df.loc[df['column'].isin(values)]

```

### Setting Multi-Level Indexes

- Indexes can be made out of multiple columns, forming a multi-level index or hierarchical index.
- Multi-level indexes allow for better representation of nested categorical variables.
- The syntax for manipulating multi-level indexes is different from manipulating columns.

Example:

```python
# Set a multi-level index
df.set_index(['column1', 'column2'])

# Sort by index values
df.sort_index()

# Sort by index values at a specific level
df.sort_index(level='level_name')

```

### Sorting by Index Values

- Index values can be sorted using the `.sort_index()` method.
- Sorting can be done at the overall index level or at a specific level of a multi-level index.

Example:

```python
# Sort by index values
df.sort_index()

# Sort by index values at a specific level
df.sort_index(level='level_name')
```

### Slicing Index Values

- Slicing allows for selecting consecutive elements of an index using `first:last` syntax.
- Index slicing can be done when the index is sorted using `.sort_index()`.
- Slicing can be performed at the outer level or inner levels of a multi-level index.

Example:

```python
# Slice rows from start to end
df.loc['start':'end']

# Slice rows at an inner level
df.loc[(level1_val, level2_val):]

# Slice rows at multiple levels
df.loc[(level1_val1, level2_val1):(level1_val2, level2_val2)]

```

### Slicing Time Series

- Time series data can be sliced by dates using the `.loc[]` method.
- Dates should be in ISO 8601 format, such as "yyyy-mm-dd".
- Slicing can be performed for specific date ranges.

Example:

```python
# Subset rows for a specific date range
df.loc['start_date':'end_date']

```

### Subsetting by Row/Column Number

- Rows and columns can be subsetted using the `.iloc[]` method.
- `.iloc[]` allows for selecting subsets using row and column numbers.

Example:

```python
# Subset using row and column numbers
df.iloc[row_number, column_number]

# Subset using slicing
df.iloc[start_row:end_row, start_column:end_column]

```

## Pivot Tables

### Pivot Temperature by City and Year

- Pivot tables allow for aggregating and summarizing data by multiple dimensions.
- To create a pivot table, use the `.pivot_table()` method and specify the desired columns as

index, columns, and values.

- The resulting pivot table can be explored and analyzed further.

Example:

```python
# Create a pivot table
df.pivot_table(values='value_column', index=['index_column1', 'index_column2'], columns='column_to_pivot')

# Example:
temp_by_country_city_vs_year = temperatures.pivot_table(values='avg_temp_c', index=['country', 'city'], columns='year')

```

### Subsetting Pivot Tables

- Pivot tables are DataFrames with sorted indexes, so subsetting techniques for DataFrames can be applied to them as well.
- Subsetting can be done using `.loc[]` and slicing.

Example:

```python
# Subset pivot table using .loc[]
pivot_table.loc['start_index':'end_index']

# Example:
temp_by_country_city_vs_year.loc['Egypt':'India']

# Example with multi-level index:
temp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi')]

```

### Calculating on a Pivot Table

- Further calculations can be performed on pivot tables to find meaningful insights.
- Subsetting and filtering techniques can be used on pivot tables to find specific rows or columns of interest.

Example:

```python
# Calculate mean temperature by year
mean_temp_by_year = pivot_table.mean()

# Filter for specific conditions
subset = mean_temp_by_year[mean_temp_by_year > threshold_value]

# Example:
mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()]

# Example with multi-level index:
mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()]

```

# Chapter 4 - Creating and visualizing Dataset

### Table of Contents

- Introduction
- Bar Plots
- Line Plots
- Scatter Plots
- Histograms
- Handling Missing Values
- Converting Data to DataFrame
- Exporting DataFrames

### Introduction

In this section, we will explore how to create and visualize DataFrames in pandas. We will use a dataset related to avocado sales to demonstrate various plotting techniques.

### Bar Plots

Bar plots are useful for comparing categorical and numeric variables. We will analyze avocado sales by different sizes.

1. Print the head of the avocados dataset to see the available columns.
2. Calculate the total number of avocados sold for each size group.
3. Create a bar plot to visualize the number of avocados sold by size.

```python
import pandas as pd
import matplotlib.pyplot as plt

avocados = pd.read_pickle('C:/Users/Александр/pj/DataCamp_projects/Data Scientist With Python/Data Manipulation with pandas Course 3/datasets/avoplotto.pkl')

# Print the head of the dataset
print(avocados.head())

# Calculate the total number of avocados sold by size
nb_sold_by_size = avocados.groupby("size")["nb_sold"].sum()

# Create a bar plot of the number of avocados sold by size
nb_sold_by_size.plot(kind="bar")

# Show the plot
plt.show()

```

### Line Plots

Line plots are ideal for visualizing changes in numeric variables over time. We will analyze the change in avocado sales over three years.

1. Get the total number of avocados sold on each date.
2. Create a line plot to visualize the number of avocados sold over time.

```python
import matplotlib.pyplot as plt

# Get the total number of avocados sold on each date
nb_sold_by_date = avocados.groupby("date")["nb_sold"].sum()

# Create a line plot of the number of avocados sold by date
nb_sold_by_date.plot(kind="line")

# Show the plot
plt.show()

```

### Scatter Plots

Scatter plots are useful for visualizing relationships between numerical variables. We will compare the number of avocados sold to average price.

1. Create a scatter plot with the number of avocados sold on the x-axis and the average price on the y-axis.

```python
# Scatter plot of nb_sold vs. avg_price
avocados.plot(x="nb_sold", y="avg_price", kind="scatter", title="Number of avocados sold vs. average price")

# Show the plot
plt.show()

```

### Histograms

Histograms allow us to visualize the distribution of numerical variables. We will compare the prices of conventional and organic avocados using histograms.

1. Create a histogram of the average price for conventional avocados.
2. Create a histogram of the average price for organic avocados.
3. Add a legend to the plot and show it.

```python
# Histogram of conventional average price
avocados[avocados["type"] == "conventional"]["avg_price"].hist()

# Histogram of organic average price
avocados[avocados["type"] == "organic"]["avg_price"].hist()

# Add a legend
plt.legend(["conventional", "organic"])

# Show the plot
plt.show()

```

### Handling Missing Values

Missing values can be a challenge in data analysis. We will explore techniques to handle missing values.

1. Check individual values for missing values in the avocados_2016 dataset.
2. Check each column for missing values in the avocados_2016 dataset
3. Create a bar plot to visualize the total number of missing values in each column.

```python
avocados_2016 = avocados.query("year == 2016")

# Check individual values for missing values
print(avocados_2016.isna())

# Check each column for missing values
print(avocados_2016.isna().any())

# Bar plot of missing values by variable
avocados_2016.isna().sum().plot(kind="bar")

# Show the plot
plt.show()

```

### Converting Data to DataFrame

We can create DataFrames from different data sources. We will explore two methods: list of dictionaries and dictionary of lists.

1. Create a list of dictionaries with new data.
2. Convert the list into a DataFrame.
3. Print the new DataFrame.

```python
# Create a list of dictionaries with new data
avocados_list = [
    {"date": "2019-11-03", "small_sold": 10376832, "large_sold": 7835071},
    {"date": "2019-11-10", "small_sold": 10717154, "large_sold": 8561348},
]

# Convert the list into a DataFrame
avocados_2019 = pd.DataFrame(avocados_list)

# Print the new DataFrame
print(avocados_2019)

```

### Exporting DataFrames

We can export DataFrames to different file formats for sharing and further analysis.

1. Sort the airline_totals DataFrame by the values of bumps_per_10k from highest to lowest.
2. Print the sorted DataFrame.
3. Save the sorted DataFrame as a CSV file.

```python
# Sort the airline_totals DataFrame
airline_totals_sorted = airline_totals.sort_values("bumps_per_10k", ascending=False)

# Print the sorted DataFrame
print(airline_totals_sorted)

# Save as airline_totals_sorted.csv
airline_totals_sorted.to_csv("airline_totals_sorted.csv")

```

Congratulations! You have learned various techniques for creating and visualizing DataFrames in pandas.