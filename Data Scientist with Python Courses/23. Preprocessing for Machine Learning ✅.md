# 23. Preprocessing for Machine Learning

Date: June 15, 2023

## What is Data Preprocessing?

Data preprocessing goes beyond cleaning and exploratory data analysis. It involves nves preparing the data for modeling by transforming categorical data to numeric and performing necessary operations to ensure the data is in a suitable format for analysis. In this section, we will explore various data preprocessing techniques using Pandas.

### 1st operation : explore ds :Pandas Operations for Data Preprocessing

Let's discuss some key Pandas operations commonly used for data preprocessing:

```python
import pandas as pd

# Get column names
columns = df.columns

# Get data types of columns
data_types = df.dtypes

# Get summary statistics of the data
summary_stats = df.describe()

# Remove missing data
# Drop rows with NA values
df.dropna(axis=0, thresh=1, inplace=True)

# Get the sum of null values for a specific column
null_values = df["B"].isnull().sum()

# Index for values that are not null for specific columns
not_null_index = df[df["B"].notnull()].index

# Drop specific rows
df.drop([1, 2, 3], inplace=True)

# Drop specific columns
df.drop("A", axis=1, inplace=True)

# Boolean indexing of columns
bool_indexing = df[df["B"] == 7]
```

Let's look at an example:

```python
# Taking a look at the volunteer dataset again, we want to drop rows
# where the category_desc column values are missing. We're going to do
# this using boolean indexing, by checking to see if we have any null
# values, and then filtering the dataset so that we only have rows
# with those values.

# Check how many values are missing in the category_desc column
print(volunteer["category_desc"].isnull().sum())

# Subset the volunteer dataset
volunteer_subset = volunteer[volunteer["category_desc"].notnull()]

# Print out the shape of the subset
print(volunteer_subset.shape)

```

Remember that you can use boolean indexing to effectively subset DataFrames.

### Working with Data Types
Data types play an important role in data preprocessing. Let's discuss why they are important and how to handle them using Pandas.

#### Why are Types Important?

Data types provide information about the nature of the data in a column. Some common data types include:

- `object`: Represents string or mixed types.
- `int64`: Represents integer values.
- `float64`: Represents floating-point numbers.
- `datetime64` (or `timedelta`): Represents dates or times.

#### Converting Column Types

To convert column types, you can use the `astype()` function in Pandas.

```python
# Converting a column type
# If you take a look at the volunteer dataset types, you'll see that
# the column "hits" is of type object. However, if you actually look at
# the column, you'll see that it consists of integers. Let's convert
# that column to type int.

# Print the head of the "hits" column
print(volunteer["hits"].head())

# Convert the "hits" column to type int
volunteer["hits"] = volunteer["hits"].astype("int")

# Look at the dtypes of the dataset
print(volunteer.dtypes)

```

### Class Distribution

When dealing with imbalanced class distributions, it is crucial to use appropriate techniques during train/test splitting. One such technique is stratified sampling.
#### Stratified Sampling

Stratified sampling is used when the distribution of variables in a target column is uneven. To perform stratified sampling, you can use the `train_test_split` function from scikit-learn and specify the `stratify` parameter.

- Permet de split le dataset à l’équilibre entre les categories (class distribution)

![Untitled](1.%20Final-Notes/Data%20Science/cours-data-camp/Done/23.%20Preprocessing%20for%20machine%20learning/23%20Preprocessing%20for%20Machine%20Learning%20f2668a311f364f65883f6be20e0b6ca5/Untitled.png)

```python
from sklearn.model_selection import train_test_split

# We know that the distribution of variables in the "category_desc"
# column in the volunteer dataset is uneven. If we wanted to train a
# model to predict "category_desc," we would want to train the model
# on a sample of data that is representative of the entire dataset.
# Stratified sampling is a way to achieve this.

# Create a dataset with all columns except "category_desc"
volunteer_X = volunteer.drop("category_desc", axis=1)

# Create a "category_desc" labels dataset
volunteer_y = volunteer[["category_desc"]]

# Use stratified sampling to split up the dataset
X_train, X_test, y_train, y_test = train_test_split(
    volunteer_X, volunteer_y, stratify=volunteer_y
)

# Print out the category_desc counts on the training labels
print(y_train["category_desc"].value_counts())

```

## Standardizing Data

Standardizing data is essential when using models that assume normally distributed data and have linear characteristics. Standardization involves transforming data to have a mean of zero and unit variance. This transformation brings features to a similar scale and helps capture relative changes and magnitudes of change.

- Have to be used when there is high variance
- when there are different scales (price house, number of bedroom)

```python
# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

knn = KNeighborsClassifier()

# Fit the knn model to the training data
knn.fit(X_train, y_train)

# Score the model on the test data
print(knn.score(X_test, y_test))

# Output 0.688 
```

#### Log Normalization

Log normalization is a technique used to normalize features with high variance relative to other features. It applies a logarithmic transformation to these features, bringing them closer to a normal distribution.

```python
import numpy as np

# Log normalization in Python
# Let's say we have a column "Proline" in our wine dataset that has a
# large amount of variance. We can apply log normalization to this column.

# Print out the variance of the "Proline" column
print(wine["Proline"].var())

# Apply log normalization to the "Proline" column
wine["Proline_log"] = np.log(wine["Proline"])

# Check the variance of the normalized "Proline" column
print(wine["Proline_log"].var())

```

## Scaling Data for Feature Comparison

Feature scaling is necessary when features are on different scales. Scaling brings features to a similar range, allowing for fair feature comparison. It is especially useful in models with linear characteristics.

- Scaling = transform to have normal distribution
- Mean of 0 and var of 1

```python
from sklearn.preprocessing import StandardScaler

# Scaling data - standardizing columns
# Let's say we have the "Ash," "Alcalinity of ash," and "Magnesium"
# columns in our wine dataset, which are all on different scales.
# We can standardize these columns to use them in a linear model.

# Create the scaler
ss = StandardScaler()

# Take a subset of the DataFrame you want to scale
wine_subset = wine[["Ash", "Alcalinity of ash", "Magnesium"]]

# Apply the scaler to the DataFrame subset
wine_subset_scaled = ss.fit_transform(wine_subset)

```

## Standardized Data and Modeling

Let's compare the accuracy of a model using non-scaled data and scaled data.

```python
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# KNN on non-scaled data
# Split the dataset and labels into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Fit the K-nearest neighbors model to the training data
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Score the model on the test data
print(knn.score(X_test, y_test))

```

```python
# KNN on scaled data
# Apply scaling to the dataset used for modeling
X_scaled = ss.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)

# Fit the K-nearest neighbors model to the training data
knn.fit(X_train, y_train)

# Score the model on the test data
print(knn.score(X_test, y_test))

```

Standardizing the data often leads to improved model accuracy, especially when dealing with models that assume normally distributed data and have linear characteristics.

By applying these data preprocessing techniques, you can enhance the quality and suitability of your data for modeling tasks.

## **Feature engineering**

Feature engineering involves the creation of new features based on existing features, providing insights into the relationships between features, and expanding the data. It is a dataset-dependent process that is tailored to specific scenarios. Some common scenarios for feature engineering include working with text data, categorical data, time stamps, and averages.

*Example:*

**Encoding categorical variables**

Encoding categorical variables is the process of transforming categorical data into a numerical format that can be used for modeling. There are various techniques available for encoding categorical variables.

In Pandas, you can use the `.apply()` function along with a lambda function to encode binary values. For one-hot encoding of variables with two or more labels, you can use the `.get_dummies()` function.

In scikit-learn, you can use the `LabelEncoder` class for encoding binary variables.

```python
# Encoding categorical variables - binary
# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
hiking["Accessible_enc"] = enc.fit_transform(hiking["Accessible"])

# Compare the two columns
print(hiking[["Accessible_enc", "Accessible"]].head())

# Encoding categorical variables - one-hot
# Transform the category_desc column
category_enc = pd.get_dummies(volunteer["category_desc"])

# Take a look at the encoded columns
print(category_enc.head())
```

**Engineering numerical features**

Engineering numerical features involves creating new features based on numerical data. This can include aggregation techniques and extracting information from dates.

For example, you can take an average of columns using the `.apply()` function, lambda functions, and the `.mean()` method.

You can also work with dates by converting them to datetime format using the `.to_datetime()` method in Pandas and extracting specific components such as month using lambda functions.

```python
# Engineering numerical features - taking an average
# Create a list of the columns to average
run_columns = ["run1", "run2", "run3", "run4", "run5"]

# Use apply to create a mean column
running_times_5k["mean"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)

# Take a look at the results
print(running_times_5k)

# Engineering numerical features - datetime
# First, convert string column to date column
volunteer["start_date_converted"] = pd.to_datetime(volunteer["start_date_date"])

# Extract just the month from the converted column
volunteer["start_date_month"] = volunteer["start_date_converted"].apply(lambda row: row.month)

# Take a look at the converted and new month columns
print(volunteer[["start_date_converted", "start_date_month"]].head())

```

**Text classification**

Text classification involves working with text data to extract useful features for modeling. This can include text extraction using regular expressions and TF/IDF vectorization.

For text extraction, regular expressions are used to extract specific patterns or information from text. The `re` module in Python provides functions for working with regular expressions. For TF/IDF vectorization, the `TfidfVectorizer` class from scikit-learn is commonly used to convert text into numerical features.

```python
# Engineering features from strings - extraction
# Write a pattern to extract numbers and decimals
def return_mileage(length):
    pattern = re.compile(r"\\d+\\.\\d+")
    # Search the text for matches
    mile = re.match(pattern, length)
    # If a value is returned, use group(0) to return the found value
    if mile is not None:
        return float(mile.group(0))

# Apply the function to the Length column and

 take a look at both columns
hiking["Length_num"] = hiking["Length"].apply(lambda row: return_mileage(row))
print(hiking[["Length", "Length_num"]].head())

# Engineering features from strings - tf/idf
# Take the title text
title_text = volunteer["title"]

# Create the vectorizer method
tfidf_vec = TfidfVectorizer()

# Transform the text into tf-idf vectors
text_tfidf = tfidf_vec.fit_transform(title_text)

```

**Feature selection**

Feature selection involves selecting relevant features to be used in modeling. It aims to improve the model's performance by removing noisy or redundant features.

Some techniques for feature selection include removing redundant features and checking for correlated features.

```python
# Removing redundant features
# Create a list of redundant column names to drop
to_drop = ["category_desc", "created_date", "locality", "region", "vol_requests"]

# Drop those columns from the dataset
volunteer_subset = volunteer.drop(to_drop, axis=1)

# Print out the head of the new dataset
print(volunteer_subset.head())

# Checking for correlated features
# Print out the column correlations of the wine dataset
print(wine.corr())

# Take a minute to find the column where the correlation value is greater than 0.75 at least twice
to_drop = "Flavanoids"

# Drop that column from the DataFrame
wine = wine.drop(to_drop, axis=1)

```

Feature selection is an iterative process, and different combinations of features may need to be tried to optimize the model.

## Selecting Features Using Text Vectors

### Looking at Word Weights

```python
print(tfidf_vec.vocabulary_)
print(text_tfidf[3].data)
print(text_tfidf[3].indices)

```

### Exploring Text Vectors, Part 1

Let's expand on the text vector exploration method we just learned about, using the volunteer dataset's title tf/idf vectors. In this first part of text vector exploration, we're going to add to the function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector.

```python
# Add in the rest of the parameters
def return_weights(vocab, original_vocab, vector, vector_index, top_n):
    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))

    # Let's transform that zipped dict into a series
    zipped_series = pd.Series({vocab[i]: zipped[i] for i in vector[vector_index].indices})

    # Let's sort the series to pull out the top n weighted words
    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index
    return [original_vocab[i] for i in zipped_index]

# Print out the weighted words
print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))
```

### Exploring Text Vectors, Part 2

Using the function we wrote in the previous exercise, we're going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.

```python
def words_to_filter(vocab, original_vocab, vector, top_n):
    filter_list = []
    for i in range(0, vector.shape[0]):
        # Here we'll call the function from the previous exercise and extend the list we're creating
        filtered = return_weights(vocab, original_vocab, vector, i, top_n)
        filter_list.extend(filtered)
    # Return the list in a set, so we don't get duplicate word indices
    return set(filter_list)

# Call the function to get the list of word indices
filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)

# By converting filtered_words back to a list, we can use it to filter the columns in the text vector
filtered_text = text_tfidf[:, list(filtered_words)]

```

### Training Naive Bayes with Feature Selection

Let's re-run the Naive Bayes text classification model we ran at the end of chapter 3, with our selection choices from the previous exercise, on the volunteer dataset's title and category_desc columns.

```python
# Split the dataset according to the class distribution of category_desc
train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)

# Fit the model to the training data
nb.fit(train_X, train_y)

# Print out the model's accuracy
print(nb.score(test_X, test_y))

```

# ****Selecting Features for Modeling****

- do not create new features
- select features in dataset
- sk learn has automatic selection of features

### Removing redundant features

- It is possible to remove redundant features by understanding the dataset
    - remove city name, state because we already have information concerning the city (lat, long)
    - we can drop values when we calculated the mean of col

```python
# Create a list of redundant column names to drop
to_drop = ["category_desc", "created_date", "locality", "region", "vol_requests"]

# Drop those columns from the dataset
volunteer_subset = volunteer.drop(to_drop, axis=1)

# Print out the head of volunteer_subset
print(volunteer_subset.head())
```

- Correlated features
    - Statistically correlated

```python
# Print out the column correlations of the wine dataset
print(wine.corr())

# Drop that column from the DataFrame
wine = wine.drop('Flavanoids', axis=1)

print(wine.head())
```

### Selection features using text vectors

![Untitled](1.%20Final-Notes/Data%20Science/cours-data-camp/Done/23.%20Preprocessing%20for%20machine%20learning/23%20Preprocessing%20for%20Machine%20Learning%20f2668a311f364f65883f6be20e0b6ca5/Untitled%201.png)

```python
# Add in the rest of the arguments
def return_weights(vocab, original_vocab, vector, vector_index, top_n):
    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))
    
    # Transform that zipped dict into a series
    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})
    
    # Sort the series to pull out the top n weighted words
    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index
    return [original_vocab[i] for i in zipped_index]

# Print out the weighted words
print(return_weights(
    vocab,
    tfidf_vec.vocabulary_,
    text_tfidf,
    8,
    top_n=3))

def words_to_filter(vocab, original_vocab, vector, top_n):
    filter_list = []
    for i in range(0, vector.shape[0]):
    
        # Call the return_weights function and extend filter_list
        filtered = return_weights(vocab, original_vocab, vector, i, top_n)
        filter_list.extend(filtered)
        
    # Return the list in a set, so we don't get duplicate word indices
    return set(filter_list)

# Call the function to get the list of word indices
filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)

# Filter the columns in text_tfidf to only those in filtered_words
filtered_text = text_tfidf[:, list(filtered_words)]

# Split the dataset according to the class distribution of category_desc
X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)

# Fit the model to the training data
nb.fit(X_train, y_train)

# Print out the model's accuracy
print(nb.score(X_test, y_test))

# output : 0.516. You can see that our accuracy score wasn't 
# that different from the score at the end of Chapter 3. 
# But don't worry, this is mainly because of how small the title field is.
```

### Dimensionality Reduction and PCA

Dimensionality reduction is like trying to fit a big puzzle into a smaller puzzle, where the small puzzle has fewer pieces but still shows the big picture. In this case, the puzzle is data and the pieces are different types of information we have about it. Sometimes we have too much information and it's hard to understand, so we try to simplify it by taking only the most important pieces. PCA is one way to do this, where we combine similar pieces of information to create new pieces that are easier to understand. It's like taking two colors of paint and mixing them together to make a new color that shows both of their qualities.

Same method than above but less manual.

- Unsupervised learning
- Combines/decomposes a feature space
- Feature extraction used to reduce feature space
- Principal Component Analysis (PCA)
- Linear transformation to an uncorrelated space
- Captures as much variance as possible in each component

### PCA Caveats

- Difficult to interpret components
- End of preprocessing journey

### Using PCA

Let's apply PCA to the wine dataset to see if we can increase our model's accuracy.

```python
from sklearn.decomposition import PCA

# Instantiate a PCA object
pca = PCA()

# Define the features and labels from the wine dataset
X = wine.drop('Type', axis=1)
y = wine["Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# Apply PCA to the wine dataset X vector
pca_X_train = pca.fit_transform(X_train)
pca_X_test = pca.transform(X_test)

# Look at the percentage of variance explained by the different components
print(pca.explained_variance_ratio_)
```

### Training a Model with PCA

Now that we have run PCA on the wine dataset, let's try training a model with it.

```python
# Split the transformed X and the y labels into training and test sets
X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)

# Fit knn to the training data
knn.fit(X_wine_train, y_wine_train)

# Score knn on the test data and print it out
print(knn.score(X_wine_test, y_wine_test))

```

### UFOs and Preprocessing

### Checking Column Types

Take a look at the UFO dataset's column types using the `dtypes` attribute. Two columns jump out for transformation: the `seconds` column, which is a numeric column but is being read in as an object, and the `date` column, which can be transformed into the `datetime` type. That will make our feature engineering efforts easier later on.

```python
# Check the column types
print(ufo.dtypes)

# Change the type of seconds to float
ufo["seconds"] = ufo["seconds"].astype(float)

# Change the date column to type datetime
ufo["date"] = pd.to_datetime(ufo["date"])

# Check the column types
print(ufo[["seconds", "date"]].dtypes)

```

### Dropping Missing Data

Let's remove some of the rows where certain columns have missing values. We're going to look at the `length_of_time` column, the `state` column, and the `type` column. If any of the values in these columns are missing, we're going to drop the rows.

```python
# Check how many values are missing in the length_of_time, state, and type columns
print(ufo[["length_of_time", "state", "type"]].isnull().sum())

# Keep only rows where length_of_time, state, and type are not null
ufo_no_missing = ufo[ufo["length_of_time"].notnull() &
                     ufo["state"].notnull() &
                     ufo["type"].notnull()]

# Print out the shape of the new dataset
print(ufo_no_missing.shape)

```

### Categorical Variables and Standardization

### Extracting Numbers from Strings

The `length_of_time` field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from the text field using regular expressions.

```python
def return_minutes(time_string):
    # We'll use \\d+ to grab digits and match it to the column values
    pattern = re.compile(r"\\d+")

    # Use match on the pattern and column
    num = re.match(pattern, time_string)
    if num is not None:
        return int(num.group(0))

# Apply the extraction to the length_of_time column
ufo["minutes"] = ufo["length_of_time"].apply(return_minutes)

# Take a look at the head of both of the columns
print(ufo[["length_of_time", "minutes"]].head())
```

### Identifying Features for Standardization

In this section, you'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the `seconds` and `minutes` columns, you'll see that the variance of the `seconds` column is extremely high. Because `seconds` and `minutes` are related to each other (an issue we'll deal with when we select features for modeling), let's log normalize the `seconds` column.

```python
# Check the variance of the seconds and minutes columns
print(ufo[["seconds", "minutes"]].var())

# Log normalize the seconds column
ufo["seconds_log"] = np.log(ufo["seconds"])

# Print out the variance of just the seconds_log column
print(ufo["seconds_log"].var())

```

### Encoding Categorical Variables

There are a couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You'll do that transformation here, using both binary and one-hot encoding methods.

```python
# Use Pandas to encode "us" values as 1 and others as 0
ufo["country_enc"] = ufo["country"].apply(lambda val: 1 if val == "us" else 0)

# Print the number of unique type values
print(len(ufo["type"].unique()))

# Create a one-hot encoded set of the type values
type_set = pd.get_dummies(ufo["type"])

# Concatenate this set back to the ufo DataFrame
ufo = pd.concat([ufo, type_set], axis=1)

```

### Features from Dates

Another feature engineering task to perform is month and year extraction. Perform this task on the `date` column of the UFO dataset.

```python
# Look at the first 5 rows of the date column
print(ufo["date"].head())

# Extract the month from the date column
ufo["month"] = ufo["date"].apply(lambda row: row.month)

# Extract the year from the date column
ufo["year"] = ufo["date"].apply(lambda row: row.year)

# Take a look at the head of all three columns
print(ufo[["date", "month", "year"]].head())

```

### Text Vectorization

Let's transform the `desc` column in the UFO dataset into tf/idf vectors since there's likely something we can learn from this field.

```python
# Take a look at the head of the desc field
print(ufo["desc"].head())

# Create the tfidf vectorizer object
vec = TfidfVectorizer()

# Use vec's fit_transform method on the desc field
desc_tfidf = vec.fit_transform(ufo["desc"])

# Look at the number of columns this creates.
print(desc_tfidf.shape)

```

### Selecting the Ideal Dataset

Let's get rid of some of the unnecessary features. Because we have an encoded `country` column (`country_enc`), keep it and drop other columns related to location: `city`, `country`, `lat`, `long`, `state`. We have columns related to month and year, so we don't need the `date` or `recorded` columns. We vectorized `desc`, so we don't need it anymore. For now, we'll keep `type`. We'll keep `seconds_log` and drop `seconds` and `minutes`. Let's also get rid of the `length_of_time` column, which is unnecessary after extracting minutes.

```python
# Check the correlation between the seconds, seconds_log, and minutes columns
print(ufo[["seconds", "seconds_log", "minutes"]].corr())

# Make a list of features to drop
to_drop = ["city", "country", "date", "desc", "lat", "length_of_time", "long", "minutes", "recorded", "seconds", "state"]

# Drop those features
ufo_dropped = ufo.drop(to_drop, axis=1)

# Let's also filter some words out of the text vector we created
filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)

```

### Modeling the UFO Dataset, Part 1

In this exercise, we're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our `X` dataset has the log-normalized `seconds` column, the one-hot encoded `type` columns, as well as the month and year when the sighting took place. The `y` labels are the encoded `country` column, where 1 represents the US and 0 represents Canada.

```python
# Take a look at the features in the X set of data
print(X.columns)

# Split the X and y sets using train_test_split, setting stratify=y
train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)

# Fit knn to the training sets
knn.fit(train_X, train_y)

# Print the score of knn on the test sets
print(knn.score(test_X, test_y))

```

### Modeling the UFO Dataset, Part 2

Finally, let's build a model using the text vector we created, `desc_tfidf`, using the `filtered_words` list to create a filtered text vector. Let's see if we can predict the type of the sighting based on the text. We'll use a Naive Bayes model for this.

```python
# Use the list of filtered words we created to filter the text vector
filtered_text = desc_tfidf[:, list(filtered_words)]

# Split the X and y sets using train_test_split, setting stratify=y
train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)

# Fit nb to the training sets
nb.fit(train_X, train_y)

# Print the score of nb on the test sets
print(nb.score(test_X, test_y))

```

### Congratulations!

Congratulations on completing the course! As you can see, feature selection and preprocessing are crucial steps in building effective machine learning models. Keep exploring and experimenting with different techniques to improve your models.