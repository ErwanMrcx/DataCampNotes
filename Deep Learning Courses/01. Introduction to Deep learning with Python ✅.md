Link to original note: {{link}}
Date: 23/08/2023

-----
**Table of Contents**

- [[#1. Forward propagation|1. Forward propagation]]
	- [[#1. Forward propagation#1.1. Activation fonction|1.1. Activation fonction]]
	- [[#1. Forward propagation#1.2. Deeper Networks|1.2. Deeper Networks]]
- [[#2. Optimizing a neural network with backward propagation|2. Optimizing a neural network with backward propagation]]
	- [[#2. Optimizing a neural network with backward propagation#2.1. Back-Propagation|2.1. Back-Propagation]]
	- [[#2. Optimizing a neural network with backward propagation#2.2. Loss Function|2.2. Loss Function]]
	- [[#2. Optimizing a neural network with backward propagation#2.3. Gradient Descent|2.3. Gradient Descent]]
	- [[#2. Optimizing a neural network with backward propagation#2.4. Learning Rate|2.4. Learning Rate]]
	- [[#2. Optimizing a neural network with backward propagation#2.5. Slope Calculation|2.5. Slope Calculation]]
	- [[#2. Optimizing a neural network with backward propagation#2.6. Code to calculate the slopes and updates weights|2.6. Code to calculate the slopes and updates weights]]



# 1. Forward propagation

![[Pasted image 20230823112435.png]]
- Multiply - add process 
- Dot product
- Forward propagation for one data point at a time 
- Output is the prediction for that data point

Chaque noeud est multiplié par son poids, ce qui nous donne des corrélations entre chaque variable. Ensuite on repète le meme processus sur chaque "noeud caché" pour donné un output qui est la prédiction.

```python
import numpy as np

input_data = np.array([-1, 2])

weights = {
    'node_0': [3, 3],
    'node_1': [1, 5],
    'output': [2, -1]
}

# Calculate node 0 value: node_0_value
node_0_value = (input_data * weights['node_0']).sum()

# Calculate node 1 value: node_1_value
node_1_value = (input_data * weights['node_1']).sum()

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_value, node_1_value])

# Calculate output: output
output = (hidden_layer_outputs * weights['output']).sum()

# Print output
print(output)
```



## 1.1. Activation fonction
Pour maximiser l'efficience du modèle, on doit appliquer une 'activation fonction'. Elle permet de capturer les non-linéarités. Elle est appliqué aux inputs.

### 1.1.1. ReLU (Rectified Linear Activation)
ReLu est composé de droite linéaire et permet d'appliquer les activations fonction aux modèles.

La fonction est simple:
- si input est négatif alors 0
- si input est positif alors input
``` python
import numpy as np

input_data = np.array([-1, 2])

weights = {
    'node_0': [3, 3],
    'node_1': [1, 5],
    'output': [2, -1]
}

def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    # returns 0 if the number is negative, returns the number if it is positive
    output = max(input, 0)
    
    # Return the value just calculated
    return(output)

# Calculate node 0 value: node_0_output
node_0_input = (input_data * weights['node_0']).sum()
node_0_output = relu(node_0_input)

# Calculate node 1 value: node_1_output
node_1_input = (input_data * weights['node_1']).sum()
node_1_output = relu(node_1_input)

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_output, node_1_output])

# Calculate model output (do not apply relu)
model_output = (hidden_layer_outputs * weights['output']).sum()

# Print model output
# without the activation function a negative value would be printed
print(model_output)
# 1.23
```

## 1.2. Deeper Networks
- Deep Networks consist of more than 1 hidden layers. The same forward propagation process is used across all the hidden layers.
- Deep networks internally build representations of patterns in the data.
- They partially replace the need for feature engineering since subsequent layers build better representations of the raw data.
- For example, the first hidden layer may identify a line, the second may identify a diagonal, the third may identify a square, and subsequent layers may come up with more complex geometrical shapes based on the data.
- An advantage is that the modeler need not specify these interactions.

Comme les noeuds sont calculés avec ReLU, certains noeuds valent 0.
- (-5 * 5) + (3 * 4) = -13 donc = 0

[![deep network](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/raw/master/images/deeper_networks.PNG)](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/blob/master/images/deeper_networks.PNG)

**how representations learning works?**
- It will goes step by step: the first hidden layer may identify a line, the second may identify a diagonal, the third may identify a square, and subsequent layers may come up with more complex geometrical shapes based on the data.
- An advantage is that the modeler need not specify these interactions.

![[Pasted image 20230823120428.png]]
```python
import numpy as np

input_data = np.array([3, 5])

weights = {
    'node_0_0': [2, 4],
    'node_0_1': [4, -5],
    'node_1_0': [-1, 2],
    'node_1_1': [1, 2],
    'output': [2, 7]
}

def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    # returns 0 if the number is negative, returns the number if it is positive
    output = max(input, 0)

    # Return the value just calculated
    return(output)

def predict_with_network(input_data):
    # Calculate node 0 in the first hidden layer
    node_0_0_input = (input_data * weights['node_0_0']).sum()
    node_0_0_output = relu(node_0_0_input)

    # Calculate node 1 in the first hidden layer
    node_0_1_input = (input_data * weights['node_0_1']).sum()
    node_0_1_output =  relu(node_0_1_input)

    # Put node values into array: hidden_0_outputs
    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])
    
    # Calculate node 0 in the second hidden layer
    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()
    node_1_0_output = relu(node_1_0_input)

    # Calculate node 1 in the second hidden layer
    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()
    node_1_1_output = relu(node_1_1_input)

    # Put node values into array: hidden_1_outputs
    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])

    # Calculate model output: model_output
    model_output = (hidden_1_outputs * weights['output']).sum()
    
    # Return model_output
    return(model_output)

output = predict_with_network(input_data)
print(output)
```

# 2. Optimizing a neural network with backward propagation
## 2.1. Back-Propagation
- This technique propagates the error from the output through the hidden layers to the input layer.
- It allows gradient descent to update all weights in a neural network.

**Process:**
- Estimate the slope of the loss function w.r.t each weight.
- Use forward propagation to calculate the predictions and errors before doing back propagation.
- Go back one layer at a time.
- Calculate the gradient as the product of the three terms described in the above section.
- Need to keep track of the slopes w.r.t the values of the nodes.
- Slope of node values = sum of the slopes of the weights coming out from the node.

[![back propagation](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/raw/master/images/back_propagation.PNG)](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/blob/master/images/back_propagation.PNG)
**Back propagation: Recap**
- start at some random set of weights
- Use forward propagation to make a prediction
- Use backward propagation to calculate the slope of the loss function w.r.t each weight
- Multiply that slope by learning rate and substract from current weights

 **Stochastic Gradient Descent
- it is common to calculates slopes only on a subset of data.
- Use a different batch of data to calculate the next update.
- Start over from the beginning once all the data is used.
- Each time through the training data is called an **Epoch**.


**- The output of a neural network depends on the weights assigned at each input at each layer.**
- A change in weights may change the outputs to make more accurate predictions.
- Making accurate predictions gets more difficult with more data points.
- At any set of weights there may be many values of the error corresponding to the data points the predictions are made for.

Here on the graph, the output number is kwown. We change the weight to improve prediction.
![[Pasted image 20230823152854.png]]
## 2.2. Loss Function
On peut calculer la performance sur modèle avec [[MSE]]. En 3D, cela peut se présenter sous cette forme:
![[Pasted image 20230823153240.png]]

- A function that aggregates the errors in predictions from many data points into a single number. It gives a measure of the model's predictive performance.
- Low value of the loss function indicates a better model performance.
- Example: Mean Squared Error - squares all the errors and calculates the mean.

**Goal of Optimization: Find weights such that we get the lowest value of the loss function, i.e. the error is minimised.**

## 2.3. Gradient Descent
- A popular optimization algorithm is the Gradient Descent.
- Following are the steps of this algorithm:
	1. Start at a random point.
	2. Calculate the slope at this point.
	3. Move in a direction opposite the slope.
	4. Repeat until the slope is somewhat flat.
- This algorithm eventually leads to the minimum value. Hence, it is used to minimise the value of the loss function.

**Voilà comment gradient descent fonctionne:**
Si la tangente à la courbe est = à 0 alors l'intersection de ce point est le minimum de la courbe. La pente de la courbe tangente peut être calculé grâce à la dérivée de la fonction
[![gradient descent](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/raw/master/images/gradient_descent.PNG)](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/blob/master/images/gradient_descent.PNG)
```python
import numpy as np

# The data point you will make a prediction for
input_data = np.array([0, 3])

# Sample weights
weights_0 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 1]
            }

# The actual target value, used to calculate the error
target_actual = 3

def predict_with_network(input_data_row, weights):

    # Calculate node 0 value
    node_0_output = (input_data_row * weights['node_0']).sum()

    # Calculate node 1 value
    node_1_output = (input_data_row * weights['node_1']).sum()

    # Put node values into array: hidden_layer_outputs
    hidden_layer_outputs = np.array([node_0_output, node_1_output])
    
    # Calculate model output
    model_output = (hidden_layer_outputs * weights['output']).sum()
    
    # Return model output
    return(model_output)

# Make prediction using original weights
model_output_0 = predict_with_network(input_data, weights_0)

# Calculate error: error_0
error_0 = model_output_0 - target_actual

# Create weights that cause the network to make perfect prediction (3): weights_1
# changed the weight of node_1 to 1 and 0.

weights_1 = {'node_0': [2, 1],
             'node_1': [1, 0],
             'output': [1, 1]
            }

# Make prediction using new weights: model_output_1
model_output_1 = predict_with_network(input_data, weights_1)

# Calculate error: error_1
error_1 = model_output_1 - target_actual

# Print error_0 and error_1
print(error_0)
print(error_1)
```

## 2.4. Learning Rate
- If the slope is positive: going opposite to the slope means, moving to lower numbers, i.e. Subtracting the slope from the current value.
- In this, the problem is that, if the step is too big, we may move far astray (overshooting minima).
- Hence, we multiply the slope by a small value called learning rate.
- Update the current value by subtracting (learning rate x slope) from the current value.

## 2.5. Slope Calculation
Consider an example:

```
    2
3------->6
```

Here the input 3, feeds into the output with a weight of 2. Let, Actual Target=10 Here, the prediction is 6. Error = 6-10 = -4

La pente peut être calculé en multipliant:
- La valeur du noeud, ici 2
- la valeur La pente de la loss fonction en l'occurrence (predicted - Actual) = (6 - 10) = -4
- La valeur du noeud, ici 3
- La pente de la fonction d'activation que nous n'avons pas utilisé ici
- résulat = 2 * -4 * 3 

Hence, the slope is given as:
```
Slope=2 * (-4) * 3= -24
```

Now, since we have the slope, we can calculate the new weight. Consider the learning rate=0.01
```
New Weight = Old Value - (learning_rate * Slope)
```

New weight = 2 - (0.01 * -24) = 2.24

This process is repeated for each weight. In the end, all the weights are updated _simultaneously_.

## 2.6. Code to calculate the slopes and updates weights
```python
n_updates = 20
mse_hist = []

# Iterate over the number of updates
for i in range(n_updates):

# Calculate the slope: slope
slope = get_slope(input_data, target, weights)

# Update the weights: weights
weights = weights - 0.01 * slope

# Calculate mse with new weights: mse
mse = get_mse(input_data, target, weights)

# Append the mse to mse_hist
mse_hist.append(mse)

# Plot the mse history
plt.plot(mse_hist)
plt.xlabel('Iterations')
plt.ylabel('Mean Squared Error')
plt.show()
```


# Building deep learning models with keras
The keras workflow has 4 steps:
1. Specify the architecture - number of layers, number of nodes in each layer, activation function
2. Compile the model
3. Fit the model on the training data
4. Use the model to make predictions

## Model specification with Sequencial()
After you read the data into an np array, specify the number of columns (data.shape[1]). This gives the number of nodes in the input layer.

You can specify the type of model to be used:

```python
import numpy as np
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential 

# Import what we need
predictors = np.loadtxt('data.csv', delimiter=',')
n_cols = predictors.shape[1]

model = Sequential()
```

You can add layers to the network using the add() method.
```python

# Builiding the models specification
model.add(Dense(100, activation='relu', input_shape=(n_cols, )))
model.add(Dense(100, activation='relu'))
model.add(Dense(1))
```

Here, Dense means that every node in the previous layer is connected to every node in the next layer.

100 is the number of nodes in the layer. 'activation' specifies the activation function to be used. 
- The first line specifies the input layer. 
- The next line specifies a hidden layer. 
- The last line specifies the output layer with a single node.

## Compile the model 
The compile method in Keras has two different arguments:

1. The optimizer to be used
	- This controls the learning rate.
	- The most commonly used optimizer is the **"Adam"**. It adjusts the weights as gradient descent is performed.
1. Loss Function
	- The most commonly used function is the mean_squared_error

```python
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(predictors, target)
```

**Optimizer est une forme de journal montrant les performance du modèle sur les données d'apprentissage lorsque nous mettons les poids à jour.**

## Classification models 

For classification models,
- The most commonly used loss function used is **categorical_crossentropy**. This is similar to a log loss function. A lower value indicates a better model.
- add `metrics=['accuracy']` is used in the compile step for easy-to-understand diagnostics.
- The output layer has a different node for each possible outcome and uses 'softmax' activation. This ensures that the predictions are interpreted as probabilities.

```python
# Import necessary modules
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.utils import to_categorical

# Convert the target to categorical: target
# survived is a categorical target field in the pre-loaded data in df
# to_categorical performs one-hot encoding on the column
target = to_categorical(df.survived)

# Set up the model
model = Sequential()

# Add the first layer
model.add(Dense(32, activation='relu', input_shape=(n_cols,)))

# Add the output layer
model.add(Dense(2, activation='softmax'))

# Compile the model
# optimizer- stochastic gradient descent
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model
model.fit(predictors, target)
```

## Saving and reloading models using Keras

- When you build a model, you need to save the model. Models are saved in a file having format HDF - Hierarchical Data Format.
- This model can then be reloaded for use.
- Using the reloaded model, you can make predictions.
```python
from keras.models import load_model
model.save('my_model.h5')
mymodel = load_model('my_model.hf)
pred = mymodel.predict(data)
```

This can be useful in avoiding rebuilding a model or verifying the structure of the model before making predictions.

# Fine-tuning keras models
## Why is Optimization hard?

- The optimal value for any weight depends on the values of the other weights.
- It involves optimizing 1000s of parameters simulataneously with complex relationships.
- Updates may not improve the model meaningfully.

Updates may be too small if the learning rate used is too small and they may be too large if the learning rate is too large. A smart optimizer like 'Adams' helps but these problems may still persist.

## Using different Optimizers
You can tune the learning rate in the optimizer being used. For example, if you are using Stochastic Gradient Descent,

```python
learning_rate = 0.01
my_optimizer = SGD(lr=lr)
model.compile(optimizer = my_optimizer, loss ='categorical_crossentropy')
```

## Problems in Optimization
### Dying Neuron Problem

This problem occurs when a neuron outputs a value of 0 for all the rows in the data.
- In case of a relu activation function, this would mean, the weight coming into that node is negative.
- Hence, the slop of the weights as well as the output is 0. This means, the weights do not get updated.
- Such a neuron has nothing to add to the model and is hence called _dead_.

[![dying neuron](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/raw/master/images/dying_neuron.PNG)](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/blob/master/images/dying_neuron.PNG)
At first, we may feel that an activation function which never outputs an exact zero could be a solution. However, this may result in another problem called Vanishing gradients.
### Vanishing Gradients
Earlier, a popular activation function was used called the _tanh_ function.

[![vanishing gradient](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/raw/master/images/vanishing_gradient.png)](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/blob/master/images/vanishing_gradient.png)
- This problem occurs when many layers have very small slopes due to being on the flat part of the tanh curve.
- Small slopes may work in networks with very few layers, however, with deep networks, the slopes become close to 0. This implies that in backpropagation, the updates are close to 0.
- Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function

For the vanishing gradient problem, the first thought would be to use an activation function that does not go flat anywhere. Lot of reasearch has been to evaluate the effects of each of these problems and their solutions.

## Model Validation

Validation data is the data that is held out explicitly from the training data and is used only to test the model performance. A common technique is k-fold cross validation. In practice, k-fold cross validation on deep learning is very computationally expensive.

Keras makes it easy to use some data as validation data by specifying a parameter in the fit() method.

```python 
model.fit(predictors, target, validation_split=0.3)
```

The above statement specifies that 30% of the data should be used as validation data.The idea is to have a best possible validation of the model. Hence, we keep training as long as the model is improving. We stop once the validation no longer gives better results. This can be achieved with the help of **Early Stopping**.

```python
from keras.callbacks import EarlyStopping

early_stopping_monitor = EarlyStopping(patience=2)
model.fit(predictors, target, validation_split=0.3, nb_epoch=20, callbacks=[early_stopping_monitor])
```

The parameter _patience_ specifies how many epochs the model can go without training before we stop training any further.

By default, keras trains for 10 epochs. Since we have specified when to stop, we can have a higher number of epochs which is given by nb_epoch=20.

```python
# Import EarlyStopping
from tensorflow.keras.callbacks import EarlyStopping

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]
input_shape = (n_cols,)

# Specify the model
model = Sequential()
model.add(Dense(100, activation='relu', input_shape = input_shape))
model.add(Dense(100, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience=2)

# Fit the model
model.fit(predictors, target, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])
```

## Optimize model example 
```python
# The input shape to use in the first hidden layer
input_shape = (n_cols,)

# Create the new model: model_2
model_2 = Sequential()

# Add the first, second, and third hidden layers
model_2.add(Dense(10, activation='relu', input_shape=input_shape))
model_2.add(Dense(10, activation='relu'))
model_2.add(Dense(10, activation='relu'))

# Add the output layer
model_2.add(Dense(2, activation='softmax'))

# Compile model_2
model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit model 1
model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)

# Fit model 2
model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)

# Create the plot
plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')
plt.xlabel('Epochs')
plt.ylabel('Validation score')
plt.show()
```

The plot compare the 2 models. We will choose the one with the lowest loss values

## Model Capacity

Under fitting: quand le modèle ne permet pas prédire correctement que ce soit sur le training data, new dataset ou validation data 

Over fitting: quand le modèle permet de prédire sur le training dataset mais incapable sur le validation dataset

- We can experiment with different models to see which one yields bettwen results.
- This experimentation can be based on number of layers, type of model, validation, activation function, etc.
- For this, being aware of the model capacity is important. This term is closely related to Overfitting and Underfitting.
- Model capacity is a model's ability to capture predictive patterns in the data.
- Capacity can be in terms of layers, number of nodes in the layers, etc.

[![model capacity](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/raw/master/images/mode_capacity.png)](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/blob/master/images/mode_capacity.png)

### Workflow for optimizing model capacity
- Start with a simple network and get the validation score.
- Gradually add capacity.
- Keep increasing capacity till the validation score is no longer improving.

Example:
[![workflow](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/raw/master/images/capacity_optimizing_workflow.PNG)](https://github.com/shwetajoshi601/datacamp-deep-learning-in-python/blob/master/images/capacity_optimizing_workflow.PNG)

